## 超大数据量去重（10亿级别以上）

给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?

思路：

假如每个url大小为10bytes，那么可以估计每个文件的大小为50G×64=320G，
远远大于内存限制的4G，所以不可能将其完全加载到内存中处理，可以采用分治的思想来解决。

- Step1：

  遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000
  个小文件(记为a0,a1,...,a999，每个小文件约300M);
  
- Step2:

  遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999);
  
  >巧妙之处：这样处理后，所有可能相同的url都被保存在对应的小文件(a0 vs b0,a1 vs b1,...,a999 vs b999)中，
  >不对应的小文件不可能有相同的url。然后我们只要求出这个1000对小文件中相同的url即可

- Step3：

  求每对小文件ai和bi中相同的url时，可以把ai的url存储到hash_set/hash_map中。
  然后遍历bi的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
  

**图解**

![](./asserts/分而治之思想.png)

**拓展思考**

怎么对10亿个电话号码进行去重？
























